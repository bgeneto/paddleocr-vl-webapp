# vLLM Configuration for PaddleOCR-VL
# Optimized for high-throughput PDF processing
#
# Common fixes for "No available memory for cache blocks":
# 1. Increase gpu_memory_utilization (up to 0.98)
# 2. Reduce max_num_seqs (try 32 or 16)
# 3. Reduce max_model_len (try 2048)
# 4. Enable enforce_eager (disable CUDA graphs)
#
# NOTE: vLLM uses underscores in YAML config keys, not hyphens!

# GPU memory fraction to use (0.0-1.0)
# Higher = more memory for model and KV cache = better batching
# 0.90 is aggressive but safe for dedicated GPU
gpu_memory_utilization: 0.55

# Maximum concurrent sequences - KEY for parallel page processing!
# Higher = more pages processed simultaneously
# Match or exceed MAX_PARALLEL_PAGES for best throughput
max_num_seqs: 64

# Maximum context length
# PaddleOCR-VL supports up to 32768 tokens
# 8192 is good for most documents
max_model_len: 16384

# Disable CUDA graphs (uses less memory but slower)
# Keep false for best performance if you have enough VRAM
enforce_eager: false

# Maximum batched tokens - controls throughput
# Higher = more efficient GPU utilization
# Should be >= max_model_len for optimal batching
max_num_batched_tokens: 65536

# Enable chunked prefill for better memory efficiency with long contexts
enable_chunked_prefill: true
# Speculative decoding disabled (not needed for OCR)
# num_speculative_tokens: 0
